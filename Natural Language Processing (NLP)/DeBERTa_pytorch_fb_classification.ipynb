{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":11889935,"sourceType":"datasetVersion","datasetId":7473260}],"dockerImageVersionId":31090,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import torch\nfrom torch.utils.data import Dataset, DataLoader\nfrom torch.optim import AdamW\nfrom transformers import DebertaForSequenceClassification, AutoTokenizer, get_scheduler\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import f1_score\nimport pandas as pd\nimport numpy as np\nimport re","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-16T13:36:31.401326Z","iopub.execute_input":"2025-07-16T13:36:31.402018Z","iopub.status.idle":"2025-07-16T13:36:31.406106Z","shell.execute_reply.started":"2025-07-16T13:36:31.401995Z","shell.execute_reply":"2025-07-16T13:36:31.405533Z"}},"outputs":[],"execution_count":15},{"cell_type":"code","source":"# CLEAN THE DATA\ndef clean_text(text):\n    text = text.lower()\n    text = re.sub(r\"http\\S+\", \"\", text)\n    text = re.sub(r\"[^a-z\\s]\", \"\", text)\n    text = re.sub(r\"\\s+\", \" \", text).strip()\n    return text","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-16T13:36:32.089829Z","iopub.execute_input":"2025-07-16T13:36:32.090445Z","iopub.status.idle":"2025-07-16T13:36:32.094433Z","shell.execute_reply.started":"2025-07-16T13:36:32.090422Z","shell.execute_reply":"2025-07-16T13:36:32.093697Z"}},"outputs":[],"execution_count":16},{"cell_type":"code","source":"# LOAD LABELED DATA\ndf = pd.read_csv(\"/kaggle/input/fb-post-classification/FB_posts_labeled.txt\", sep=\"\\t\")\n\ndef get_label(row):\n    if row[\"Appreciation\"] == 1:\n        return \"Appreciation\"\n    elif row[\"Complaint\"] == 1:\n        return \"Complaint\"\n    else:\n        return \"Feedback\"\n\ndf[\"label\"] = df.apply(get_label, axis=1)\ndf[\"clean_text\"] = df[\"message\"].astype(str).map(clean_text)\n\nlabel_encoder = LabelEncoder()\nencoded_labels = label_encoder.fit_transform(df[\"label\"])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-16T13:36:39.346033Z","iopub.execute_input":"2025-07-16T13:36:39.346306Z","iopub.status.idle":"2025-07-16T13:36:39.591584Z","shell.execute_reply.started":"2025-07-16T13:36:39.346286Z","shell.execute_reply":"2025-07-16T13:36:39.590820Z"}},"outputs":[],"execution_count":17},{"cell_type":"code","source":"# TRAIN/VAL SPLIT\ntrain_texts, val_texts, train_labels, val_labels = train_test_split(\n    df[\"clean_text\"].tolist(), encoded_labels, test_size=0.2, stratify=encoded_labels, random_state=452\n)\n\nmodel_name = \"microsoft/deberta-base\"\ntokenizer = AutoTokenizer.from_pretrained(model_name)\n\ntrain_tokens = tokenizer(train_texts, truncation=True, padding=True, max_length=64, return_tensors=\"pt\")\nval_tokens   = tokenizer(val_texts,   truncation=True, padding=True, max_length=64, return_tensors=\"pt\")\n\ntrain_labels = torch.tensor(train_labels)\nval_labels = torch.tensor(val_labels)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-16T13:36:40.903499Z","iopub.execute_input":"2025-07-16T13:36:40.904027Z","iopub.status.idle":"2025-07-16T13:36:41.928497Z","shell.execute_reply.started":"2025-07-16T13:36:40.904002Z","shell.execute_reply":"2025-07-16T13:36:41.927717Z"}},"outputs":[],"execution_count":18},{"cell_type":"code","source":"# DATASET\nclass FBCommentDataset(Dataset):\n    def __init__(self, tokens, labels):\n        self.tokens = tokens\n        self.labels = labels\n\n    def __len__(self):\n        return len(self.labels)\n\n    def __getitem__(self, idx):\n        return {\n            \"input_ids\": self.tokens[\"input_ids\"][idx],\n            \"attention_mask\": self.tokens[\"attention_mask\"][idx],\n            \"labels\": self.labels[idx],\n        }\n\ntrain_dataset = FBCommentDataset(train_tokens, train_labels)\nval_dataset   = FBCommentDataset(val_tokens, val_labels)\n\ntrain_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\nval_loader   = DataLoader(val_dataset, batch_size=16)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-16T13:36:43.219072Z","iopub.execute_input":"2025-07-16T13:36:43.219643Z","iopub.status.idle":"2025-07-16T13:36:43.280359Z","shell.execute_reply.started":"2025-07-16T13:36:43.219616Z","shell.execute_reply":"2025-07-16T13:36:43.279638Z"}},"outputs":[],"execution_count":19},{"cell_type":"code","source":"# MODEL SETUP\nmodel = DebertaForSequenceClassification.from_pretrained(model_name, num_labels=3)\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nmodel.to(device)\n\noptimizer = AdamW(model.parameters(), lr=5e-5, weight_decay=0.01)\nnum_epochs = 3\nnum_training_steps = len(train_loader) * num_epochs\n\nlr_scheduler = get_scheduler(\n    name=\"linear\",\n    optimizer=optimizer,\n    num_warmup_steps=0,\n    num_training_steps=num_training_steps,\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-16T13:36:44.699294Z","iopub.execute_input":"2025-07-16T13:36:44.699600Z","iopub.status.idle":"2025-07-16T13:36:45.513014Z","shell.execute_reply.started":"2025-07-16T13:36:44.699579Z","shell.execute_reply":"2025-07-16T13:36:45.512371Z"}},"outputs":[{"name":"stderr","text":"Some weights of DebertaForSequenceClassification were not initialized from the model checkpoint at microsoft/deberta-base and are newly initialized: ['classifier.bias', 'classifier.weight', 'pooler.dense.bias', 'pooler.dense.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"}],"execution_count":20},{"cell_type":"code","source":"# TRAINING LOOP\nfor epoch in range(num_epochs):\n    model.train()\n    total_loss = 0\n\n    for batch in train_loader:\n        batch = {k: v.to(device) for k, v in batch.items()}\n        outputs = model(**batch)\n        loss = outputs.loss\n        total_loss += loss.item()\n\n        loss.backward()\n        optimizer.step()\n        lr_scheduler.step()\n        optimizer.zero_grad()\n\n    avg_loss = total_loss / len(train_loader)\n    print(f\"Epoch {epoch+1} complete. Avg loss: {avg_loss:.4f}\")\n\n    # VALIDATION EVALUATION\n    model.eval()\n    val_preds = []\n    val_trues = []\n\n    with torch.no_grad():\n        for batch in val_loader:\n            batch = {k: v.to(device) for k, v in batch.items()}\n            outputs = model(**batch)\n            logits = outputs.logits\n            preds = torch.argmax(logits, dim=1)\n\n            val_preds.extend(preds.cpu().numpy())\n            val_trues.extend(batch[\"labels\"].cpu().numpy())\n\n    f1 = f1_score(val_trues, val_preds, average=\"macro\")\n    print(f\"Validation F1 Score (macro): {f1:.4f}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-16T13:36:50.952602Z","iopub.execute_input":"2025-07-16T13:36:50.952938Z","iopub.status.idle":"2025-07-16T13:42:07.431587Z","shell.execute_reply.started":"2025-07-16T13:36:50.952893Z","shell.execute_reply":"2025-07-16T13:42:07.430765Z"}},"outputs":[{"name":"stdout","text":"Epoch 1 complete. Avg loss: 0.4569\nValidation F1 Score (macro): 0.8670\nEpoch 2 complete. Avg loss: 0.2180\nValidation F1 Score (macro): 0.8635\nEpoch 3 complete. Avg loss: 0.0910\nValidation F1 Score (macro): 0.8744\n","output_type":"stream"}],"execution_count":21},{"cell_type":"code","source":"# EVALUATE ON UNLABELED DATA\n\n# Define new dataset for unlabeled inputs\nclass UnlabeledDataset(Dataset):\n    def __init__(self, tokens):\n        self.tokens = tokens\n\n    def __len__(self):\n        return self.tokens[\"input_ids\"].shape[0]\n\n    def __getitem__(self, idx):\n        return {\n            \"input_ids\": self.tokens[\"input_ids\"][idx],\n            \"attention_mask\": self.tokens[\"attention_mask\"][idx]\n        }\n\n# Tokenize\nunlabeled_df = pd.read_csv(\"/kaggle/input/fb-post-classification/FB_posts_unlabeled.txt\", sep=\"\\t\")\nunlabeled_df[\"clean_text\"] = unlabeled_df[\"message\"].astype(str).map(clean_text)\n\nunlabeled_tokens = tokenizer(\n    unlabeled_df[\"clean_text\"].tolist(),\n    truncation=True,\n    padding=True,\n    max_length=64,\n    return_tensors=\"pt\"\n)\n\n# Create DataLoader\nunlabeled_dataset = UnlabeledDataset(unlabeled_tokens)\nunlabeled_loader = DataLoader(unlabeled_dataset, batch_size=32)\n\n# Run inference in batches\nmodel.eval()\nall_preds = []\n\nwith torch.no_grad():\n    for batch in unlabeled_loader:\n        batch = {k: v.to(device) for k, v in batch.items()}\n        outputs = model(**batch)\n        logits = outputs.logits\n        preds = torch.argmax(logits, dim=1)\n        all_preds.extend(preds.cpu().numpy())\n\n# One-hot encode and save\none_hot = torch.nn.functional.one_hot(torch.tensor(all_preds), num_classes=3).numpy()\n\npred_df = pd.DataFrame({\n    \"postId\": unlabeled_df[\"postId\"],\n    \"Appreciation_pred\": one_hot[:, 0],\n    \"Complaint_pred\":    one_hot[:, 1],\n    \"Feedback_pred\":     one_hot[:, 2]\n})\n\npred_df.to_csv(\"predictions.csv\", index=False)\nprint(f\"Wrote predictions.csv with {len(pred_df)} rows.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-16T13:45:32.525066Z","iopub.execute_input":"2025-07-16T13:45:32.525666Z","iopub.status.idle":"2025-07-16T13:45:42.392096Z","shell.execute_reply.started":"2025-07-16T13:45:32.525643Z","shell.execute_reply":"2025-07-16T13:45:42.391270Z"}},"outputs":[{"name":"stdout","text":"Wrote predictions.csv with 2039 rows.\n","output_type":"stream"}],"execution_count":23}]}
