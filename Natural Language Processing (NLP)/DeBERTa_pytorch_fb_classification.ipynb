{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-16T13:36:31.402018Z",
     "iopub.status.busy": "2025-07-16T13:36:31.401326Z",
     "iopub.status.idle": "2025-07-16T13:36:31.406106Z",
     "shell.execute_reply": "2025-07-16T13:36:31.405533Z",
     "shell.execute_reply.started": "2025-07-16T13:36:31.401995Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.optim import AdamW\n",
    "from transformers import DebertaForSequenceClassification, AutoTokenizer, get_scheduler\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import f1_score\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-16T13:36:32.090445Z",
     "iopub.status.busy": "2025-07-16T13:36:32.089829Z",
     "iopub.status.idle": "2025-07-16T13:36:32.094433Z",
     "shell.execute_reply": "2025-07-16T13:36:32.093697Z",
     "shell.execute_reply.started": "2025-07-16T13:36:32.090422Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# CLEAN THE DATA\n",
    "def clean_text(text):\n",
    "    text = text.lower()\n",
    "    text = re.sub(r\"http\\S+\", \"\", text)\n",
    "    text = re.sub(r\"[^a-z\\s]\", \"\", text)\n",
    "    text = re.sub(r\"\\s+\", \" \", text).strip()\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-16T13:36:39.346306Z",
     "iopub.status.busy": "2025-07-16T13:36:39.346033Z",
     "iopub.status.idle": "2025-07-16T13:36:39.591584Z",
     "shell.execute_reply": "2025-07-16T13:36:39.590820Z",
     "shell.execute_reply.started": "2025-07-16T13:36:39.346286Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# LOAD LABELED DATA\n",
    "df = pd.read_csv(\"FB_posts_labeled.txt\", sep=\"\\t\")\n",
    "\n",
    "def get_label(row):\n",
    "    if row[\"Appreciation\"] == 1:\n",
    "        return \"Appreciation\"\n",
    "    elif row[\"Complaint\"] == 1:\n",
    "        return \"Complaint\"\n",
    "    else:\n",
    "        return \"Feedback\"\n",
    "\n",
    "df[\"label\"] = df.apply(get_label, axis=1)\n",
    "df[\"clean_text\"] = df[\"message\"].astype(str).map(clean_text)\n",
    "\n",
    "label_encoder = LabelEncoder()\n",
    "encoded_labels = label_encoder.fit_transform(df[\"label\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-16T13:36:40.904027Z",
     "iopub.status.busy": "2025-07-16T13:36:40.903499Z",
     "iopub.status.idle": "2025-07-16T13:36:41.928497Z",
     "shell.execute_reply": "2025-07-16T13:36:41.927717Z",
     "shell.execute_reply.started": "2025-07-16T13:36:40.904002Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# TRAIN/VAL SPLIT\n",
    "train_texts, val_texts, train_labels, val_labels = train_test_split(\n",
    "    df[\"clean_text\"].tolist(), encoded_labels, test_size=0.2, stratify=encoded_labels, random_state=452\n",
    ")\n",
    "\n",
    "model_name = \"microsoft/deberta-base\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "train_tokens = tokenizer(train_texts, truncation=True, padding=True, max_length=64, return_tensors=\"pt\")\n",
    "val_tokens   = tokenizer(val_texts,   truncation=True, padding=True, max_length=64, return_tensors=\"pt\")\n",
    "\n",
    "train_labels = torch.tensor(train_labels)\n",
    "val_labels = torch.tensor(val_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-16T13:36:43.219643Z",
     "iopub.status.busy": "2025-07-16T13:36:43.219072Z",
     "iopub.status.idle": "2025-07-16T13:36:43.280359Z",
     "shell.execute_reply": "2025-07-16T13:36:43.279638Z",
     "shell.execute_reply.started": "2025-07-16T13:36:43.219616Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# DATASET\n",
    "class FBCommentDataset(Dataset):\n",
    "    def __init__(self, tokens, labels):\n",
    "        self.tokens = tokens\n",
    "        self.labels = labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return {\n",
    "            \"input_ids\": self.tokens[\"input_ids\"][idx],\n",
    "            \"attention_mask\": self.tokens[\"attention_mask\"][idx],\n",
    "            \"labels\": self.labels[idx],\n",
    "        }\n",
    "\n",
    "train_dataset = FBCommentDataset(train_tokens, train_labels)\n",
    "val_dataset   = FBCommentDataset(val_tokens, val_labels)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
    "val_loader   = DataLoader(val_dataset, batch_size=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-16T13:36:44.699600Z",
     "iopub.status.busy": "2025-07-16T13:36:44.699294Z",
     "iopub.status.idle": "2025-07-16T13:36:45.513014Z",
     "shell.execute_reply": "2025-07-16T13:36:45.512371Z",
     "shell.execute_reply.started": "2025-07-16T13:36:44.699579Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DebertaForSequenceClassification were not initialized from the model checkpoint at microsoft/deberta-base and are newly initialized: ['classifier.bias', 'classifier.weight', 'pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# MODEL SETUP\n",
    "model = DebertaForSequenceClassification.from_pretrained(model_name, num_labels=3)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "optimizer = AdamW(model.parameters(), lr=5e-5, weight_decay=0.01)\n",
    "num_epochs = 3\n",
    "num_training_steps = len(train_loader) * num_epochs\n",
    "\n",
    "lr_scheduler = get_scheduler(\n",
    "    name=\"linear\",\n",
    "    optimizer=optimizer,\n",
    "    num_warmup_steps=0,\n",
    "    num_training_steps=num_training_steps,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-16T13:36:50.952938Z",
     "iopub.status.busy": "2025-07-16T13:36:50.952602Z",
     "iopub.status.idle": "2025-07-16T13:42:07.431587Z",
     "shell.execute_reply": "2025-07-16T13:42:07.430765Z",
     "shell.execute_reply.started": "2025-07-16T13:36:50.952893Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 complete. Avg loss: 0.4569\n",
      "Validation F1 Score (macro): 0.8670\n",
      "Epoch 2 complete. Avg loss: 0.2180\n",
      "Validation F1 Score (macro): 0.8635\n",
      "Epoch 3 complete. Avg loss: 0.0910\n",
      "Validation F1 Score (macro): 0.8744\n"
     ]
    }
   ],
   "source": [
    "# TRAINING LOOP\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "\n",
    "    for batch in train_loader:\n",
    "        batch = {k: v.to(device) for k, v in batch.items()}\n",
    "        outputs = model(**batch)\n",
    "        loss = outputs.loss\n",
    "        total_loss += loss.item()\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        lr_scheduler.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "    avg_loss = total_loss / len(train_loader)\n",
    "    print(f\"Epoch {epoch+1} complete. Avg loss: {avg_loss:.4f}\")\n",
    "\n",
    "    # VALIDATION EVALUATION\n",
    "    model.eval()\n",
    "    val_preds = []\n",
    "    val_trues = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in val_loader:\n",
    "            batch = {k: v.to(device) for k, v in batch.items()}\n",
    "            outputs = model(**batch)\n",
    "            logits = outputs.logits\n",
    "            preds = torch.argmax(logits, dim=1)\n",
    "\n",
    "            val_preds.extend(preds.cpu().numpy())\n",
    "            val_trues.extend(batch[\"labels\"].cpu().numpy())\n",
    "\n",
    "    f1 = f1_score(val_trues, val_preds, average=\"macro\")\n",
    "    print(f\"Validation F1 Score (macro): {f1:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-16T13:45:32.525666Z",
     "iopub.status.busy": "2025-07-16T13:45:32.525066Z",
     "iopub.status.idle": "2025-07-16T13:45:42.392096Z",
     "shell.execute_reply": "2025-07-16T13:45:42.391270Z",
     "shell.execute_reply.started": "2025-07-16T13:45:32.525643Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrote predictions.csv with 2039 rows.\n"
     ]
    }
   ],
   "source": [
    "# EVALUATE ON UNLABELED DATA\n",
    "\n",
    "# Define new dataset for unlabeled inputs\n",
    "class UnlabeledDataset(Dataset):\n",
    "    def __init__(self, tokens):\n",
    "        self.tokens = tokens\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.tokens[\"input_ids\"].shape[0]\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return {\n",
    "            \"input_ids\": self.tokens[\"input_ids\"][idx],\n",
    "            \"attention_mask\": self.tokens[\"attention_mask\"][idx]\n",
    "        }\n",
    "\n",
    "# Tokenize\n",
    "unlabeled_df = pd.read_csv(\"FB_posts_unlabeled.txt\", sep=\"\\t\")\n",
    "unlabeled_df[\"clean_text\"] = unlabeled_df[\"message\"].astype(str).map(clean_text)\n",
    "\n",
    "unlabeled_tokens = tokenizer(\n",
    "    unlabeled_df[\"clean_text\"].tolist(),\n",
    "    truncation=True,\n",
    "    padding=True,\n",
    "    max_length=64,\n",
    "    return_tensors=\"pt\"\n",
    ")\n",
    "\n",
    "# Create DataLoader\n",
    "unlabeled_dataset = UnlabeledDataset(unlabeled_tokens)\n",
    "unlabeled_loader = DataLoader(unlabeled_dataset, batch_size=32)\n",
    "\n",
    "# Run inference in batches\n",
    "model.eval()\n",
    "all_preds = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch in unlabeled_loader:\n",
    "        batch = {k: v.to(device) for k, v in batch.items()}\n",
    "        outputs = model(**batch)\n",
    "        logits = outputs.logits\n",
    "        preds = torch.argmax(logits, dim=1)\n",
    "        all_preds.extend(preds.cpu().numpy())\n",
    "\n",
    "# One-hot encode and save\n",
    "one_hot = torch.nn.functional.one_hot(torch.tensor(all_preds), num_classes=3).numpy()\n",
    "\n",
    "pred_df = pd.DataFrame({\n",
    "    \"postId\": unlabeled_df[\"postId\"],\n",
    "    \"Appreciation_pred\": one_hot[:, 0],\n",
    "    \"Complaint_pred\":    one_hot[:, 1],\n",
    "    \"Feedback_pred\":     one_hot[:, 2]\n",
    "})\n",
    "\n",
    "pred_df.to_csv(\"predictions.csv\", index=False)\n",
    "print(f\"Wrote predictions.csv with {len(pred_df)} rows.\")"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "datasetId": 7473260,
     "sourceId": 11889935,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31090,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
